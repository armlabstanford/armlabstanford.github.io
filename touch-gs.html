<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Gaussian Splatting, Monocular Depth, Gaussian Process Implicit Surface, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://armlabstanford.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://aidenswann.com/">Aiden Swann*</a>,</span>
            <span class="author-block">
              <a href="https://peasant98.github.io/">Matthew Strong*,</span>
            <span class="author-block">
              <a href="https://arm.stanford.edu/people/wonkyung-do">Won Kyung Do</a>,
            </span>
            <span class="author-block">
              <a href="https://msl.stanford.edu/people/gadielsznaiercamps">Gadi Sznaier Camps</a>,
            </span>
            <span class="author-block">
              <a href="https://web.stanford.edu/~schwager/">Mac Schwager</a>,
            </span>
            <span class="author-block">
              <a href="https://monroekennedy3.com/">Monroe Kennedy III</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Stanford University</span>
          </div>

          <!-- <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/armlabstanford/Touch-GS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/final_renders.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Touch-GS combines the power of vision and touch to generate high-quality few-shot and challenging scenes, such as few-shot object centric scenes, mirrors,
          and transparent objects.</span>
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 teaser">Abstract</h2>
        <div class="content has-text-justified teaser">
          <p>
            We present a novel method to supervise 3D Gaussian Splatting (3DGS) scenes using optical tactile sensors. 
            Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a Gaussian Process Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. 
            We merge this model with a monocular depth estimation network which is aligned in a two stage process coarsely aligning with a depth camera and then finely adjusting to match our touch data. 
            For every input color image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information we propose a new loss function, variance weighted depth supervised loss. 
            We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in a few-view scene syntheses on opaque as well as on reflective, and transparent objects.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

            
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 teaser">Method</h2>
        
        <div class="content has-text-justified teaser">
          <p>
            Our method leverages state-of-the-art monocular depth estimation and Gaussian Process Implicit Surfaces from touches along an object and optimally fuses them to train a Gaussian
            Splatting model, or any other traditional NeRF. The monocular depth estimator gives us a coarse depth map, which we then align to real-world depths
            with depth data from a noisy depth camera and further with our touch data. We then combine this with our Gaussian Process Implicit Surface, which provides a more finer depth map.
            Finally, we can use a novel, uncertainty-weighted depth loss to train a NeRF on few view scenes, as well as mirrors and transparent objects, where vision alone fails.
          </p>
            <img src="./static/images/method.png"
            alt="Method Image."
            style="width: 100%; height: auto; display: block; margin: auto;"/>
            
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 teaser">Gaussian Process Implicit Surface</h2>
        
        <div class="content has-text-justified teaser">
          <p>
            We use a Gaussian Process Implicit Surface (GPIS) to represent a touched object. The GPIS seamelessly fuses many noisy touches into a 3D representation of the object with uncertainty.
            The below images demonstrate how our GPIS is able to fill in the gaps between touches and still provide a smooth representation of the object, while still providing uncertainty.
          </p>
            <div id="banner">
              <div class="inline-block-gpis">
                  <img src ="./static/images/gpis.png">
              </div>
              <div class="inline-block-gpis-render">
                  <img src ="./static/images/gpis_render.png">
              </div>
            </div>
          
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width teaser">
        <h2 class="title is-3 ">Monocular Depth Estimation and Fusion</h2>

        <div class="content has-text-justified">
          <p>
            We perform a monocular depth estimation and alignment procedure, which consists of using the ZOEDepth monocular depth estimator to output a coarse depth.
            We then align this depth with real world data from a Realsense to learn a scale factor and offset. 
            Finally, we align this depth with our touch data with an offset. This output depth is used to compute an uncertainty map,
            which is based on a simple heuristic that the uncertainty is proportional to the depth; higher depth predictions mean higher uncertainty.
          </p>

          <div id="banner">
            <div class="inline-block-zoe">
              <img src ="./static/images/rgb.png">
            </div>
            <div class="inline-block-zoe">
                <img src ="./static/images/zoe.png">
            </div>
            <div class="inline-block-zoe">
              <img src ="./static/images/zoe_uncertainty.png">
            </div>
          </div>
          
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width teaser">
        <h2 class="title is-3 ">Method at a Glance</h2>

        <div class="content has-text-justified">
          <p>
            All combined, our method
          </p>

          <div id="banner">
            <div class="inline-block-gpis-init">
              <img src ="./static/images/method_stages.png">
            </div>

          </div>
          
        </div>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width teaser">
        <h2 class="title is-3">Results</h2>
        <p>
          We show our method 
        </p>

        <div class="content is-centered">
          DS-GS vs. Touch-GS
        </div>
        
        <div class="content has-text-justified iframe-container">
          <iframe frameborder="0" class="juxtapose" width="48%" height="500" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=4f1f8cac-e0cd-11ee-9685-5d0fb8b12f54"></iframe>
          <iframe frameborder="0" class="juxtapose" width="48%" height="500" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=0befc3b4-e0cf-11ee-9685-5d0fb8b12f54"></iframe>
        
          <!-- <iframe frameborder="0" class="juxtapose" width="100%" height="720" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=8f7587d0-e0d1-11ee-9685-5d0fb8b12f54"></iframe>
          <iframe frameborder="0" class="juxtapose" width="100%" height="720" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=d3ce1596-e0d1-11ee-9685-5d0fb8b12f54"></iframe>
        
          <iframe frameborder="0" class="juxtapose" width="100%" height="720" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=1af1fb04-e0d2-11ee-9685-5d0fb8b12f54"></iframe>
          <iframe frameborder="0" class="juxtapose" width="100%" height="720" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=3fc065e2-e0d2-11ee-9685-5d0fb8b12f54"></iframe>
          
          <iframe frameborder="0" class="juxtapose" width="40%" height="720" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=da302d6a-e0d2-11ee-9685-5d0fb8b12f54"></iframe>
          
          <iframe frameborder="0" class="juxtapose" width="40%" height="720" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=073087ec-e0d3-11ee-9685-5d0fb8b12f54"></iframe>
         -->
        </div>
      </div>
    </div>

  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/armlabstanford" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            We graciously thank <a
              href="https://nerfies.github.io">Nerfies</a> for providing the template for this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer

</body>
</html>
