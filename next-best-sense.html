<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Touch-GS combines the power of vision and touch to generate high-quality few-shot and challenging scenes, such as few-view object centric scenes, mirrors,
        and transparent objects.">
  <meta name="keywords"
    content="Gaussian Splatting, Monocular Depth, Gaussian Process Implicit Surface, NeRF, Tactile-GS, Touch-GS, Tactile, Touch, Dexterous Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var videos = document.querySelectorAll('.lazy-load-video');

      var observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            var video = entry.target;
            var source = video.querySelector('source');
            source.src = source.dataset.src;
            video.load(); // Load the video
            video.classList.add('loaded'); // Optional: mark it as loaded
            observer.unobserve(video); // Stop observing the loaded video
          }
        });
      }, {
        threshold: 0.1
      }); // Trigger when at least 10% of the video is visible

      videos.forEach(video => {
        observer.observe(video);
      });
    });
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://arm.stanford.edu/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
      </div>

    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title is-1 publication-title"><span style="color: green;">Next</span> <span
                style="color: blue;">Best</span> <span style="color: purple;">Sense</span>: Guiding Vision and Touch
              with FisherRF for 3D
              Gaussian Splatting</h1>
            <b>Stay tuned in ~1 week for the full videos, paper, and code release!</b>
            <div class="is-size-5 publication-authors">

              <span class="author-block">
                <a href="https://peasant98.github.io/">Matthew Strong*,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Jv88S-IAAAAJ&hl=en">Boshu Lei*</a>,
              </span>
              <span class="author-block">
                <a href="https://aidenswann.com/">Aiden Swann</a>,</span>
              <span class="author-block">
                <a href="https://jiangwenpl.github.io/">Wen Jiang</a>,
              </span>
              <span class="author-block">
                <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,
              </span>
              <span class="author-block">
                <a href="https://monroekennedy3.com/">Monroe Kennedy III</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Stanford University, University of Pennsylvania</span>
              <br>
              <br>
              <a href="https://arm.stanford.edu/" style="display: inline-block; width: 30%; margin-right: 20px;">
                <img src="./static/images/armlab-logo.png" alt="Armlab Logo" style="width: 100%;">
              </a>
              <a href="https://www.grasp.upenn.edu/" style="display: inline-block; width: 20%;">
                <img src="./static/nextbestsense/grasp.svg" alt="GRASP Logo" style="width: 100%;">
              </a>
              <br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2403.09875.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.09875" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <!-- <span class="link-block">
                      <a href="https://www.youtube.com/watch?v=FqejaTEt7aU"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fab fa-youtube"></i>
                        </span>
                        <span>Video</span>
                      </a>
                    </span> -->
                <!-- <span class="link-block">
                      <a href="https://github.com/armlabstanford/Touch-GS"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                        </a>
                    </span> -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- <span class="link-block">
                <a href="https://github.com/armlabstanford/Touch-GS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 teaser">Abstract</h2>
          <div class="content has-text-justified teaser">
            <p>
              We propose a framework for active next best view and touch selection for robotic manipulators using 3D
              Gaussian Splatting (3DGS).
              3DGS is emerging as a useful explicit 3D scene representation for robotics, as it has the ability to
              represent scenes in a both
              photorealistic and geometrically accurate manner. However, in real-world online robotic scenes where the
              number of views is limited given
              efficiency requirements, random view selection for 3DGS becomes impractical as views are often overlapping
              and redundant. We address this
              issue by proposing an end-to-end online training and active view selection pipeline, which enhances the
              performance of 3DGS in few-view
              robotics settings. We first elevate the performance of few-shot 3DGS with a novel <i>semantic depth
                alignment</i> method using Segment
              Anything Model 2 (SAM2) that we supplement with Pearson depth and normal loss to improve color and depth
              reconstruction of real-world scenes.
              We then extend FisherRF, a next-best-view selection method for 3DGS, to select views and touch poses based
              on depth uncertainty. We perform
              online view selection a real robot system during live 3DGS training. We motivate our improvements to
              few-shot GS scenes,
              and extend depth-based FisherRF to these scenes, where we demonstrate both qualitative and quantitative
              improvements on challenging robot scenes.
            </p>
          </div>
        </div>
      </div>

      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">

          <div class="content has-text-justified teaser">
            <img src="./static/nextbestsense/splash_next_best.png" alt="Method Image."
              style="width: 50%; display: block; margin: auto;" />
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 teaser">Method</h2>

          <div class="content has-text-justified teaser">
            <p>
              Our method leverages state-of-the-art monocular depth
              estimation and semantic visual foundational models to train few-shot, challenging Gaussian Splatting
              scenes. We focus on
              geometrically precise reconstructions with a novel semantic depth alignment method and relative depth
              loss. We then utilize a recent work in next best view selection
              called <b>FisherRF</b>, and extend it to encode <i>depth uncertainty</i> for both vision and touch. We
              then perform online view selection on a real robot system during live 3DGS training,
              and demonstrate the effectiveness of uncertainty-guided touch on a tricky mirror scene.
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width teaser">
          <h2 class="title is-3 ">SAM2 Depth Alignment</h2>

          <div class="content has-text-justified">
            <p>
              Initialization is known to be highly sensitive in Gaussian Splatting, requiring metrically accurate
              Gaussians to seed a scene. We propose a novel
              semantic depth alignment using Segment Anything 2 to align monocular depths. Concretely, we feed a color
              image into a monocular depth estimator such as DepthAnythingV2. We then run the SAM2 automatic mask
              generator on the RGB image, which captures objects and backgrounds in the scene.
              This method does not require any depth completion networks that require extensive fine-tuning, and is a
              computationally cheap step. With a depth from a depth image, often prone to noise and incorrect in
              challenging lighting conditions,
              we perform a mask-aware alignment on the monocular depth to output a SAM2 aligned depth; retaining the
              best of both worlds: <b>metrically accurate and geometrically precise</b>.
            </p>

            <div id="banner">
              <div class="inline-block-sam2-init">
                <img src="./static/nextbestsense/semantic_alignment.png">
              </div>
            </div>

            <p>
              The alignment is used as initialization for the scene. SAM2 aligned depths (<i>right</i>) improves
              significantly upon a least squares alignment (<i>left</i>) monocular depth.

            </p>

            <div id="banner">
              <div class="inline-block-sam2-init">
                <img style="width: 60%; display: block; margin: auto;" src="./static/nextbestsense/sam2-mesh.png">
              </div>
            </div>

          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width teaser">
          <h2 class="title is-3 ">Geometric Guidance</h2>

          <div class="content has-text-justified">
            <p>
              We utilize Pearson depth loss and normal supervision to guide the Splat into a realistic scene.
            </p>

            <div id="banner">
              <div class="inline-block-sam2-init">
                <img src="./static/nextbestsense/pearson.png">
              </div>
            </div>

          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width teaser">
          <h2 class="title is-3">Few-Shot Gaussian Splatting Results</h2>
          <p>
            We show our method for few-shot GS in a variety of scenarios
            The below consists of a <b>RGB</b> and <b>depth</b>
            comparison between lifted least-squares-aligned depths and semantically aligned depths.
            Our method is on the right side of the slider.
          </p>

          <div class="content is-centered gs-comp">
            <b> Bunny Blender Scene (6 input views)</b>
          </div>

          <div class="content has-text-justified iframe-container">
            <iframe frameborder="0" class="juxtapose-t" width="48%"
              src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=4d254ca8-7c5d-11ef-9397-d93975fe8866"></iframe>
            <iframe frameborder="0" class="juxtapose-t" width="48%"
              src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=d9f6a068-7c5e-11ef-9397-d93975fe8866"></iframe>
          </div>

          <div class="content is-centered gs-comp">
            <b> Bunny Real Scene (8 input views)</b>
          </div>

          <div class="content has-text-justified iframe-container">
            <iframe frameborder="0" class="juxtapose" width="48%"
              src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=73db6ee0-7c62-11ef-9397-d93975fe8866"></iframe>
            <iframe frameborder="0" class="juxtapose" width="48%"
              src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=4e22ff92-7c62-11ef-9397-d93975fe8866"></iframe>
          </div>

          <p>
            Even on a challenging prism object, our SAM2-based method is able to capture the scene densely compared to
            prior works that use only geometric depth guidance.

          </p>

          <div class="content is-centered gs-comp">
            <b> Prism Real Scene (single training view)</b>
          </div>

          <div class="content has-text-justified iframe-container" , style="align-items: flex-start;">
            <img src="./static/nextbestsense/prism.png" alt="Method Image." style="width: 48%;" />
            <iframe frameborder="0" class="juxtapose" width="48%"
              src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=5101307a-7c68-11ef-9397-d93975fe8866"></iframe>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 teaser">Next Best View for Robotics</h2>

          <div class="content has-text-justified teaser">
            <p>
              We turn to FisherRF, a state-of-the-art next best view selection method for 3DGS, and extend it to encode depth uncertainty for vision and touch. 
              This proves to be more impactful than measuring uncertainty from color alone, as depth is a more reliable measure of scene geometry. Then, in an online manner, we can 
              select views based on depth uncertainty to progressively refine a scene.
            </p>

            <div id="banner">
              <div class="inline-block-sam2-init">
                <img src="./static/nextbestsense/nbv.png" style="width: 200%;">
              </div>
            </div>

          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 teaser">Offline FisherRF</h2>

          <div class="content has-text-justified teaser">
            <p>
              We demonstrate view selection randomly, with traditional FisherRF, and with modifications to FisherRF with our improved few-shot GS. 
            </p>

            <div>
              <!-- this div includes the images for fisherRF random vs rgb vs depth -->

            </div>

          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 teaser">Online FisherRF</h2>

          <div class="content has-text-justified teaser">
            <p>
              We demonstrate our method operating in end to end fashion on a real robot system.
            </p>

          </div>
        </div>
      </div>


      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 teaser">Next Best Touch</h2>

          <div class="content has-text-justified teaser">
            <p>
              We extend our method to touch!
            </p>

          </div>
        </div>
      </div>

    </div>

    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{strong2024nextbestsense,
  author    = {Matthew Strong and Boshu Lei and Aiden Swann and Wen Jiang and Kostas Daniilidis and Monroe Kennedy III},
  title     = {Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian Splatting},
  journal   = {arXiv},
  year      = {2024},
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
        <a class="icon-link" href="https://github.com/armlabstanford" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              We graciously thank <a href="https://nerfies.github.io">Nerfies</a> for providing
              the template for this website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer </body> </html>