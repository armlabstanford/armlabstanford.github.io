<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description"
      content="Dynamic Layer Detection of a Thin Silk Cloth using DenseTact Optical Tactile Sensors">
    <meta name="keywords"
      content="Tactile Sensing, Cloth Manipulation, Robotic Assistants, Layer Detection, Dynamic Sensing">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Dynamic Layer Detection of a Thin Silk Cloth using DenseTact Optical Tactile Sensors</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
    </script>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
      var videos = document.querySelectorAll('.lazy-load-video');

      var observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            var video = entry.target;
            var source = video.querySelector('source');
            source.src = source.dataset.src;
            video.load(); // Load the video
            video.classList.add('loaded'); // Optional: mark it as loaded
            observer.unobserve(video); // Stop observing the loaded video
          }
        });
      }, {threshold: 0.1}); // Trigger when at least 10% of the video is visible

      videos.forEach(video => {
        observer.observe(video);
      });
    });
  </script>

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.ico">

    <script
      src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu"
          aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <a class="navbar-item" href="https://arm.stanford.edu/">
            <span class="icon">
              <i class="fas fa-home"></i>
            </span>
          </a>
        </div>

      </div>
    </nav>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">

              <h1 class="title is-1 publication-title">Dynamic Layer Detection of a Thin Silk Cloth using DenseTact Optical
                Tactile Sensors</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://arm.stanford.edu/people/camille-chungyoun">Camille Chungyuon*</a>,
                </span>
                <span class="author-block">
                  <a href="https://arm.stanford.edu/people/ankush-dhawan">Ankush Dhawan*,
                  </span>
                  <span class="author-block">
                    <a href="https://arm.stanford.edu/people/karina-ting">Karina Ting*</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://monroekennedy3.com/">Monroe Kennedy III</a>,
                  </span>
                </div>

                <div class="is-size-5 publication-authors">
                  <span class="author-block">Stanford University</span>
                  <br>
                  <br>
                  <a href="https://arm.stanford.edu/" style="display: inline-block; width: 30%; margin-right: 20px;">
                    <img src="./static/images/armlab-logo.png" alt="Armlab Logo" style="width: 100%;">
                  </a>
                </div>



                <div class="column has-text-centered">
                  <div class="publication-links">
                    <span class="link-block">
                      <a href="https://arxiv.org/pdf/2407.05025"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2407.05025"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a href="https://youtube.com/playlist?list=PL8ig4knmSt21gqOn0FdQ4Bcm53XfKXwFU&si=WyftQOe9pFsS95GO"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            <i class="fab fa-youtube"></i>
                        </span>
                        <span>Videos</span>
                      </a>
                    </span>
                    <span class="link-block">
                        <a href="https://www.dropbox.com/scl/fo/658b6v6vojmxvq8smg8eh/ANzEmbbwN_FbzvZFgDpWhfo?rlkey=37ytlga4z49g4taos8j18f93l&st=1r165x8b&dl=0"
                           class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                              <i class="fab fa-images"></i>
                          </span>
                          <span>Data</span>
                        </a>
                      </span>

                  </div>

                </div>
              </div>
            </div>
          </div>
        </div>
      </section>

      <section class="hero teaser">
        <div class="container ">
          <div class="hero-body">
              <div class="has-text-centered">
                <img src="./static/soft_cloth/images/splash.jpg" alt="Method Image." style="width: 65%" />
              </div>

      </script>
      <h2 class="subtitle has-text-centered" style="font-family: inherit; font-size: inherit; font-weight: normal; margin-left: 15%; margin-right: 15%;">
        <span class="dnerf">Our custom gripper and cloth layer detection method can be used in complex robotic tasks. Here, it is mounted on a LoCoBot. (a) shows the DenseTact RGB images, (b) shows the model inputs of optical flow, wrench, and joint states, and (c) shows the classification results.</span>
      </h2>
      </section>

      <section class="section">
        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3 teaser">Abstract</h2>
              <div class="content has-text-justified teaser">
                <p> Cloth manipulation is an important aspect of many everyday tasks and remains a significant challenge for robots. While existing research has made strides in tasks like cloth smoothing and folding, many studies struggle from common failure modes (crumpled corners/edges, incorrect grasp configurations) that can be solved by cloth layer detection. 
                  We present a novel method for classifying the number of grasped cloth layers using a custom gripper equipped with DenseTact 2.0 optical tactile sensors. After grasping a cloth, the gripper performs an anthropomorphic rubbing motion while collecting optical flow, 6-axis wrench, and joint state data. Using this data in a transformer-based network achieves a test accuracy of 98.21% in correctly classifying the number of grasped layers, showing the effectiveness of our dynamic rubbing method. Evaluating different inputs and model architectures highlights the effectiveness of using tactile sensor information and a transformer model for this task. A comprehensive dataset of 368 labeled trials was collected and made open-source along with this paper.
                  </p>
                </div>
              </div>
            </div>

            <!--/ Abstract. -->

            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3 teaser">Contributions</h2>

                <div class="content has-text-justified teaser">
                  <ul>
                      <li>A compact, 4 DOF gripper equipped with DenseTact 2.0 sensors, capable of performing a rubbing motion between its fingers.</li>
                      <li>A dataset for cloth layer classification based on tactile sensor output. Included classes are 0, 1, 2, and 3 layers of cloth.</li>
                      <li>A transformer-based network that successfully classifies the number of cloth layers using optical flow, wrench, and joint state data taken during the gripper’s rubbing motion.</li>
                  </ul>

                  <img src="./static/soft_cloth/images/gripper_orientation.png"
                    alt="Method Image."
                    style="width: 100%; height: auto; display: block; margin: auto;" />
                </div>
              </div>
            </div>

            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3 teaser">Hardware</h2>

                <div class="content has-text-justified teaser">
                  <p>
                    The hardware setup for the gripper is shown below. The gripper is equipped with DenseTact 2.0 sensors as the fingertips and is capable of performing a rubbing motion between its fingers, measuring optical flows and net wrenches while recording its motor joint states. There are two DYNAMIXEL XL330-M288-T on each finger, chosen for their light weight and compact design, and controlled by an OpenRB-150 Arduino-compatible embedded controller. All gripper componenents communicate via ROS2 to perfrom dynamic cloth layer classification. 
                  </p>
                  <img src="./static/soft_cloth/images/gripper_cad_and_actual.png"
                    alt="Method Image."
                    style="width: 100%; height: auto; display: block; margin: auto;" />

                </div>
              </div>
            </div>

            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3 teaser">Network Architecture</h2>

                <div class="content has-text-justified teaser">
                  <p>
                    A transformer based neural network was used to classify each grasp and rubbing motion into one of the four labels (0, 1, 2, or 3 layers of grasped cloth). The model inputs are N-length time sequences of optical flow,
                    6-axis wrench, and joint state data (N=200). Extracted features from each input are concatenated and fed into a transformer
                    encoder [14], followed by fully-connected layers and a softmax function. The resulting output is the probability distribution
                    across the 4 classes (0, 1, 2 and 3 layers of cloth). Ablations to optimize for the best model architecture were completed, and are described in depth in the paper.                  </p>
                  <img src="./static/soft_cloth/images/model_arch.png"
                    alt="Method Image."
                    style="width: 100%; height: auto; display: block; margin: auto;" />

                </div>
              </div>
            </div>
            
            <h2 class="title is-3 teaser">Experiments</h2>
            <p>
              A dataset of 368 labeled trials was collected and made publically available here: ATTACH URL. Raw RGB video streams are included, while optical flow, joint state, and net wrench data is available as npz files. The README details instructions on using the data.                 </p>
            <div class="ablations-body">
              <div class="content is-centered gs-comp">
                <b> Rubbing Motion for All Labels</b>
              </div>
              <div class="videos-container">
                <div class="video-wrapper">
                  <video id="lazyVideo" class="lazy-load-video" autoplay
                    controls muted loop playsinline preload="none">
                    <source
                      data-src="./static/soft_cloth/videos/0_layer_crp_trim.mp4"
                      type="video/mp4">
                  </video>
                </div>
                <div class="video-wrapper">
                  <video id="video2" class="lazy-load-video" autoplay
                    controls muted loop playsinline preload="none">
                    <source
                      data-src="./static/soft_cloth/videos/1_layer_crp_trim.mp4"
                      type="video/mp4">
                  </video>
                </div>
              </div>
              
              <div class="videos-container">
                <div class="video-wrapper">
                  <video id="lazyVideo" class="lazy-load-video" autoplay
                    controls muted loop playsinline preload="none">
                    <source data-src="./static/soft_cloth/videos/2_layer_crp_trim.mp4"
                      type="video/mp4">
                  </video>
                </div>
                <div class="video-wrapper">
                  <video id="video2" class="lazy-load-video" autoplay
                    controls muted loop playsinline preload="none">
                    <source data-src="./static/soft_cloth/videos/3_layer_crp_trim.mp4"
                      type="video/mp4">
                  </video>
                </div>
              </div>

            <div class="ablations-body">
              <div class="content is-centered gs-comp">
                <b> Confusion Matrices</b>
              </div>

              <p>
                Based on the best architecture derived from the ablation study, confusion matrices across training epochs and test trials are presented. In testing, only one trial was misclassified of the 56 total, giving an accuracy of 98.21% on unseen data.             </p>
              

            <div class="videos-container">
              <div class="video-wrapper">
                <video id="lazyVideo" class="lazy-load-video" autoplay
                  controls muted loop playsinline preload="none">
                  <source
                    data-src="./static/soft_cloth/videos/train_confusion_matrix_vid.mp4"
                    type="video/mp4">
                </video>
              </div>
              <div class="video-wrapper">
                <video id="video2" class="lazy-load-video" autoplay
                  controls muted loop playsinline preload="none">
                  <source
                    data-src="./static/soft_cloth/videos/test_confusion_matrix_vid.mp4"
                    type="video/mp4">
                </video>
              </div>
            </div>
              
      <section class="section">
        <div class="container is-max-desktop">
          <h2 class="title is-3 teaser">Interactive 3D Plots</h2>
      
          <!-- Plot 1 -->
          <div class="iframe-container">
            <iframe src="https://drive.google.com/file/d/1ckjsQ3Cb11X2f1E0McKcY2dFpxcyDrtY/view?usp=drive_link" width="100%" height="600px" frameborder="0" allowfullscreen></iframe>
          </div>
          
          <!-- Plot 2 -->
          <div class="iframe-container">
            <iframe src="https://drive.google.com/file/d/1Qp89RGd2t5wJfTAEFwrwKPW8BSXUNTuj/view?usp=drive_link" width="100%" height="600px" frameborder="0" allowfullscreen></iframe>
          </div>
      
          <!-- Plot 3 -->
          <div class="iframe-container">
            <iframe src="https://drive.google.com/file/d/10harnLYzBrJrcCHBcLbH9xnp-yArFGqQ/view?usp=drive_link" width="100%" height="600px" frameborder="0" allowfullscreen></iframe>
          </div>
      
          <!-- Plot 4 -->
          <div class="iframe-container">
            <iframe src="https://drive.google.com/file/d/1URGvCxkIqxij7ftwoZT-rXX3lhf966ff/view?usp=drive_link" width="100%" height="600px" frameborder="0" allowfullscreen></iframe>
          </div>
        </div>
      </section>     

      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>@article{guptasarma2024a,
      title={{ProACT}: An Augmented Reality Testbed for Intelligent Prosthetic Arms}, 
      author={Shivani Guptasarma and Monroe Kennedy},
      year={2024},
      eprint={2407.05025},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2407.05025}, 
}</code></pre>
        </div>

      
      </section>

    

      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
            <a class="icon-link" href="https://github.com/armlabstanford"
              class="external-link" disabled>
              <i class="fab fa-github"></i>
            </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p>
                  We thank <a
                    href="https://nerfies.github.io">Nerfies</a> for providing
                  the template for this website.
                </p>
              </div>
            </div>
          </div>
        </div>
        </footer

      </body>
    </html>
